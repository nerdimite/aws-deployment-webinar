{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Keras or TensorFlow model using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load the Keras model using the json and weights file\n",
    "\n",
    "Use the upload feature in jupyter to upload your model.json and model weights\n",
    "\n",
    "If you saved your model in the TensorFlow ProtoBuf format, skip to \"Step 4. Convert the TensorFlow model to an Amazon SageMaker-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Export the Keras model to the TensorFlow ProtoBuf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This directory structure will need to be followed\n",
    "model_version = '1'\n",
    "export_dir = 'export/Servo/' + model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Protocol Buffer SavedModel at 'export_dir'\n",
    "builder = builder.SavedModelBuilder(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    }
   ],
   "source": [
    "# Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "signature = predict_signature_def(\n",
    "    inputs={\"inputs\": model.input}, outputs={\"score\": model.output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: export/Servo/1/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    # Save the meta graph and variables\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=sess, tags=[tag_constants.SERVING], signature_def_map={\"serving_default\": signature})\n",
    "    builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Convert TensorFlow model to a SageMaker readable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the TensorFlow exported model into a directory export\\Servo\\. SageMaker will recognize this as a loadable TensorFlow model. Your directory and file structure should look like:\n",
    "\n",
    "```\n",
    "-export/\n",
    "    |--Servo/\n",
    "        |--{model version as an integer eg. 1}/\n",
    "            |--variables/\n",
    "            |--saved_model.pb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tar the entire directory and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('export', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'model'\n",
    "\n",
    "inputs = session.upload_data(path='model.tar.gz', key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Deploy the trained model\n",
    "\n",
    "The entry_point file \"train.py\" can be an empty Python file (this is a bug)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "sagemaker_model = TensorFlowModel(model_data = 's3://' + bucket + '/model/model.tar.gz',\n",
    "                                  role = role,\n",
    "                                  framework_version = '1.12',\n",
    "                                  entry_point = 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------!CPU times: user 602 ms, sys: 45.1 ms, total: 647 ms\n",
      "Wall time: 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "\n",
    "predictor = TensorFlowPredictor(predictor.endpoint, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the SageMaker endpoint from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "with open('vocab_dict.json', 'r') as f:\n",
    "    word_to_id = json.load(f)\n",
    "\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words\n",
    "\n",
    "def preprocess_input(text, vocab_dict, maxlen=100):\n",
    "    review = review_to_words(text)\n",
    "    tokens = review.split()\n",
    "    int_tokens = [vocab_dict[token] for token in tokens]\n",
    "    \n",
    "    if len(int_tokens) >= maxlen:\n",
    "        return int_tokens[:maxlen]\n",
    "    else:\n",
    "        diff = maxlen - len(int_tokens)\n",
    "        zeros = [0 for i in range(diff)]\n",
    "        return [zeros + int_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This movie was the Best Movie I have ever seen!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_input(text, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': {'score': {'dtype': 1,\n",
       "   'tensor_shape': {'dim': [{'size': 1}, {'size': 1}]},\n",
       "   'float_val': [0.8134368062019348]}},\n",
       " 'model_spec': {'name': 'generic_model',\n",
       "  'version': {'value': 1},\n",
       "  'signature_name': 'serving_default'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke the SageMaker endpoint using a boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-tensorflow-2020-01-09-10-39-46-231'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8134368062019348\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    " \n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=predictor.endpoint, Body=json.dumps(data))\n",
    "\n",
    "result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "pred = result['outputs']['score']['floatVal'][0]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Setting up AWS Lambda and API Gateway\n",
    "\n",
    "The following code will allow lambda to invoke the endpoint\n",
    "\n",
    "```python\n",
    "import json\n",
    "import re\n",
    "import boto3\n",
    "\n",
    "# define some preprocessing steps\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "# We now load our word_to_id dictionary from json\n",
    "with open('vocab_dict.json', 'r') as fp:\n",
    "    VOCAB_DICT = json.load(fp)\n",
    "\n",
    "# Before sending the data to the endpoint, we need to preprocess it and so\n",
    "# these functions help us do that\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words\n",
    "\n",
    "def preprocess_input(text, vocab_dict, maxlen=100):\n",
    "    review = review_to_words(text)\n",
    "    tokens = review.split()\n",
    "    int_tokens = [vocab_dict[token] for token in tokens]\n",
    "    \n",
    "    if len(int_tokens) >= maxlen:\n",
    "        return int_tokens[:maxlen]\n",
    "    else:\n",
    "        diff = maxlen - len(int_tokens)\n",
    "        zeros = [0 for i in range(diff)]\n",
    "        return [zeros + int_tokens]\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    # Preprocess the data\n",
    "    data = preprocess_input(event['body'], VOCAB_DICT)\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = 'sagemaker-tensorflow-2020-01-09-07-58-25-420',\n",
    "                                       Body = json.dumps(data)) \n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    pred = result['outputs']['score']['floatVal'][0]\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'application/json', 'Access-Control-Allow-Origin' : '*', \"Access-Control-Allow-Credentials\" : True },\n",
    "        'body' : pred\n",
    "    }\n",
    "    \n",
    "```\n",
    "\n",
    "Once you setup the lambda function, we need to connect Lambda to API Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Clean up\n",
    "\n",
    "To avoid incurring unnecessary charges, use the AWS Management Console to delete the resources that you created like the endpoint, S3 bucket, models, endpoint_configurations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
